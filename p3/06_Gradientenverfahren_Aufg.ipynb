{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradientenverfahren\n",
    "\n",
    "In diesem Aufgabenblatt geht es darum, das Gradientenverfahren mithilfe der NumPy Funktionen in Python zu implementieren.\n",
    "\n",
    "Wir verwenden einen synthetischen Datensatz mit nur einer unabhängigen Variablen auf den wir ein univariates Regressionsmodell anwenden.\n",
    "\n",
    "In der folgenden Code-Zelle importieren wir die Testdaten `X` sowie die Labels `y`. Die Ausgabe zeigt die ersten 10 Zeilen des Datensatzes.\n",
    "Wie Sie sehen, ist `X` bereits um eine Spalte von Einsen erweitert, damit wir den Bias-Parameter zusammen mit den Merkmals-Parametern behandeln können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "size = 100000\n",
    "mux = 0\n",
    "stdx = 10\n",
    "X = np.random.normal(mux,stdx,size)\n",
    "shiftx = np.min(X)\n",
    "X = X.reshape(size,1)\n",
    "stdnoise = 10.2\n",
    "noise = np.random.normal(0,stdnoise,size)\n",
    "noise = noise.reshape(size,1)\n",
    "y = 12 + 4.4*X + noise\n",
    "x0 = (np.ones(size)).reshape(size,1)\n",
    "X = np.concatenate((x0, X), 1)\n",
    "\n",
    "print(\"X = \", X[0:10,])\n",
    "print(\"y = \", y[0:10,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellfunktion, Kostenfunktion und Normalengleichung für die lineare Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren nun 3 elementare Funktionen für die lineare Regression:\n",
    "Die Modellfunktion `h`, die Kostenfunktion `J` und die Normalengleichung.\n",
    "\n",
    "Bevor wir jedoch in die Programmierung dieser Funktionen einsteigen, nochmal einige Hinweise zu Python und NumPy.\n",
    "Da wir für die Daten NumPy Arrays verwenden, können wir die Matrixoperationen von NumPy nutzen.\n",
    "`A.T` etwa liefert die **transponierte** Matrix von `A`.\n",
    "`A@B` berechnet die Matrixmultiplikation von `A` und `B`.\n",
    "Sollten Sie sich einmal nicht sicher sein, wie viele Zeilen und Spalten eine Matrix `A` besitzt, können Sie sich mit `np.shape(A)` die Dimensionen anzeigen lassen.\n",
    "\n",
    "Auch wenn das Ergebnis `A` eine Matrixoperation ein einzelner Wert ist, ist der Datentyp des Resultats eine `1x1`-Matrix.\n",
    "Um an den Wert zu gelangen, müssen Sie auf `A[0,0]` oder `A[0][0]` zugreifen.\n",
    "Alternativ können Sie mit Alternativ können Sie mit `A.item(i)` den `i`-ten Wert aus der Matrix abfragen.\n",
    "\n",
    "Um eine Matrix `A` zu invertieren, benutzen Sie die NumPy-Funktion `inv()` aus dem Paket `linalg`.\n",
    "Wir benutzen diese Funktion bei der Berechnung der Normalgleichung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Modellfunktion `h` auszurechnen, müssen wir alle Werte im Merkmalsvektor mit den entsprechenden Modellparametern multiplizieren und die Ergebnisse aufsummieren.\n",
    "Dies entspricht dem Skalarprodukt (*dot product*) der beiden Vektoren.\n",
    "\n",
    "Betrachten wir nochmal unsere Daten-Matrix $X$.\n",
    "in den $m$ Zeilen von $X$ sind einzelne Datenpunkte.\n",
    "Ein Datenpunkt besteht aus $n$ Werten, die in einem Merkmalsvektor $(x_1, x_2, ..., x_n)$ zusammengefasst sind.\n",
    "Zur Vereinfachung tragen wir eine $1$ an die erste Position des Vektors ein. Diesen Wert bezeichnen wir mit $x_0$, er dient zur einfacheren Berechnung des Bias-Terms.\n",
    "\n",
    "Wenn nun ein Merkmalsvektor $x$ ein Zeilenvektor ist, und der Parametervektor $\\theta$ ein Spaltenvektor mit gleich vielen Einträgen, kann man das Skalarprodukt als Matrixmultiplikation von $x$ und $\\theta$ berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x,theta):\n",
    "    return x@theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praktischerweise funktioniert die gleiche Funktion nuicht nur für Merkmalsvektoren, sondern die gesamte Daten-Matrix `X`.\n",
    "In diesem Fall ist das Ergebnis der Multiplikation $X\\theta$ ein Spaltenvektor $y$ mit $m$ Werten $y^{(i)}$, die den Vorhersagen der abhängigen Variablen der einzelnen Datenpunkte entsprechen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ein Beispiel: Betrachten Sie folgenden NumPy Arrays.\n",
    "`X0` ist eine $8\\times{}2$ Matrix die 8 Datenpunkte mit einem Merkmal plus den vorangestellten Einser-Vektor enthält.\n",
    "`y0` ist ein Spaltenvektor mit Beobachtungen für die abhängige Variable.\n",
    "Wir wollen in den nächsten Schritten den *verkleinerten* Datensatz aus `X0` und `y0` verwenden, um die Funktionen zu entwickeln und zu überprüfen.\n",
    "Für das Gradientenverfahren benutzen wir später den großen Datensatz aus `X` und `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.array([[1,1],[1,2],[1,3],[1,4],[1,5],[1,6],[1,7],[1,8]])\n",
    "y0 = np.array([[2],[4],[4],[6],[6],[6],[8],[8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als 2D-Plot sieht die Verteilung der Datenpunkte so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X0[:,1],y0)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.ylim(1,10)\n",
    "plt.xlim(0,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht hier schon, das eine lineare Funktion, die die Punkte recht gut beschreiben würde, den Achsenabschnitt $1$ und die Steigung $1$ hat.\n",
    "Daher ist der Vektor $(1,1)$ eine gute Schätzung für $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([[1],[1]])\n",
    "y_hat = h(X0,theta)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir im folgenden Plot sehen, liefert unser geschätztes $\\theta$ eine recht gute Modellfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X0[:,1],y0)\n",
    "plt.plot(X0[:,1],y_hat,c='r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.ylim(1,10)\n",
    "plt.xlim(0,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun müssen wir noch die Kostenfunktion $J_{MSE}(X,\\theta)$ berechnen.\n",
    "Laut Definition berechnet sich $J_{MSE}$ wie folgt:\n",
    "\n",
    "$J_{MSE}(X,\\theta)=\\frac{1}{m}\\sum_{i=1}^m ( h(x,\\theta)-y^{(i)})^2$\n",
    "\n",
    "Wir müssen also von der Vorhersage unseres Modells $h(x,\\theta)$ die tatsächlich beobachteten Werte $y^{(i)}$ abziehen, das Resultat quadrieren, die Ergebnisse für alle $m$ Datenpunkte aufadieren und normieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch in diesem Fall können wir wieder einen Trick anwenden, um die Summe zu umgehen.\n",
    "Wenn wir $h(X,\\theta)$ für die gesamte Matrix $X$ ausrechnen, bekommen wir einen kompletten *Schätzvektor* $\\hat{y}$.\n",
    "Von diesem können wir den Vektor $y$ abziehen und erhalten als Ergebnis wieder einen Vektor mit $m$ Werten.\n",
    "\n",
    "Nun müssen wir die einzelnen Werte in diesem Vektor quadrieren und aufsummieren.\n",
    "Auch hier hilft uns wieder das Skalarprodukt!\n",
    "Das Skalarprodukt $x\\cdot{}x$ multipliziert jeden Wert in $x$ mit sich selber und summiert alle Teilergebnisse auf.\n",
    "Wenn wir aus dem Spaltenvektor $x$ mit $x^T$ einen Zeilenvektor machen, können wir das Skalarprodukt als Matrixmultiplikation $x^Tx$ ausrechnen.\n",
    "\n",
    "Die Definition von $J_{MSE}(X,\\theta)$ vereinfacht sich also zu folgender Form:\n",
    "\n",
    "$J_{MSE}(X,\\theta)=\\frac{1}{m}d^Td$,    mit    $d=h(x,\\theta)-y)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_mse(x, y, theta):\n",
    "    d = h(x,theta)-y\n",
    "    r = d.T@d / len(x)\n",
    "    return r.item(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für den Datensatz `X0` und `y0` mit der Parameterschätzung $(1,1)$, liefert unsere Funktion $J_{MSE}$ folgende Kosten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_mse(X0,y0,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir eine Funktion zur Berechnung der Normalengleichung definieren.\n",
    "Mit dieser Funktion können die optimalen Parameter $\\theta$ direkt berechnet werden.\n",
    "Die Normalengleichung ist folgendermaßen definiert:\n",
    "\n",
    "$\\hat{\\theta}=(X^TX)^{-1}X^Ty$\n",
    "\n",
    "**Aufgabe: Implementieren Sie die Funktion `normalengleichung`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2637c557fd60f713",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalgleichung(X, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ae49ce36c92f9ff9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "test_X0 = np.array([[1,1],[1,2],[1,3],[1,4],[1,5],[1,6]])\n",
    "test_y0 = np.array([[10],[9],[7],[8],[6],[4]])\n",
    "#----------\n",
    "# normalgleichung\n",
    "\n",
    "assert normalgleichung(test_X0, test_y0).shape == (2,1)\n",
    "assert np.allclose(normalgleichung(test_X0, test_y0), [[11.1333],[-1.0857]], rtol = 1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frage: Welches Ergebnis liefer die Normalengleichung für unseren Datensatz `X0`, `y0`?**\n",
    "\n",
    "**Frage: War unsere Schätzung $\\theta=(1,1)$ optimal?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = normalgleichung(X0, y0)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_mse(X0,y0,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X0[:,1],y0)\n",
    "plt.plot(X0[:,1],h(X0,t),c='r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.ylim(1,10)\n",
    "plt.xlim(0,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Gradientenverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können nun die Normalengleichung verwenden um die optimalen Parameter für unseren großen Datensatz $X$ und $y$ zu bestimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Verwenden Sie die Funktion `Normalgleichung` um die optimalen Regressionsparameter $\\Theta_0$ und $\\Theta_1$ zu ermitteln.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f9472d6cf3b06a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta_norm = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "theta_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ca6af897f9466a50",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "assert np.allclose(theta_norm, [[12], [4]], rtol = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vervollständigen Sie die Funktion `gradient_descent` um die Berechnung des Gradienten $\\frac{1}{m}X^T(X\\Theta-y)$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8ed909eb4ca9d13",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grad(X, y, theta):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3319fe3acf554b9b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "assert grad(test_X0, test_y0, [[10],[-1]]).shape == (2, 1)\n",
    "assert np.allclose(grad(test_X0, test_y0, [[10],[-1]]), [[-0.8333],[-2.6667]], rtol = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterationen):\n",
    "    kosten = []\n",
    "    for iter in range(iterationen):\n",
    "        kosten.append(J_mse(X, y, theta))\n",
    "        gradient = grad(X, y, theta)\n",
    "        theta = theta - (alpha * gradient)\n",
    "    return theta, kosten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testen Sie Ihre Funktion `gradient_descent` mit dem Datensatz.**\n",
    "**Passen die Ergebnisse zu den zuvor Berechneten optimalen Parametern?**\n",
    "**Variieren Sie die Lernrate $\\alpha$ sowie die Anzahl der Epochen.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = np.random.randn(2,1)\n",
    "print(theta_0)\n",
    "\n",
    "alpha = 0.01\n",
    "theta_gd, kosten = gradient_descent(X, y, theta_0, alpha, 400)\n",
    "theta_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können nun die Lernkurve für den letzten Trainingslauf plotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.ylabel(r'$J(\\Theta)$')\n",
    "plt.xlabel(r'Epoche')\n",
    "plt.plot(range(1,len(kosten)),kosten[1:], \"x\", label=r'$\\alpha='+str(alpha))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
